@article{adamsLearningOptimizeHalide2019,
  title = {Learning to Optimize Halide with Tree Search and Random Programs},
  author = {Adams, Andrew and Ma, Karima and Anderson, Luke and Baghdadi, Riyadh and Li, Tzu-Mao and Gharbi, Micha{\"e}l and Steiner, Benoit and Johnson, Steven and Fatahalian, Kayvon and Durand, Fr{\'e}do and {Ragan-Kelley}, Jonathan},
  year = {2019},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {38},
  number = {4},
  pages = {121:1--121:12},
  issn = {0730-0301},
  doi = {10.1145/3306346.3322967},
  urldate = {2023-12-22},
  abstract = {We present a new algorithm to automatically schedule Halide programs for high-performance image processing and deep learning. We significantly improve upon the performance of previous methods, which considered a limited subset of schedules. We define a parameterization of possible schedules much larger than prior methods and use a variant of beam search to search over it. The search optimizes runtime predicted by a cost model based on a combination of new derived features and machine learning. We train the cost model by generating and featurizing hundreds of thousands of random programs and schedules. We show that this approach operates effectively with or without autotuning. It produces schedules which are on average almost twice as fast as the existing Halide autoscheduler without autotuning, or more than twice as fast with, and is the first automatic scheduling algorithm to significantly outperform human experts on average.},
  keywords = {halide,notion,optimizing compilers},
  file = {/Users/yanyucheng/Zotero/storage/6UDZW7TX/Adams et al. - 2019 - Learning to optimize halide with tree search and r.pdf}
}

@misc{baghdadiTiramisuPolyhedralCompiler2018,
  title = {Tiramisu: {{A Polyhedral Compiler}} for {{Expressing Fast}} and {{Portable Code}}},
  shorttitle = {Tiramisu},
  author = {Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Del Sozzo, Emanuele and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
  year = {2018},
  month = dec,
  number = {arXiv:1804.10694},
  eprint = {1804.10694},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1804.10694},
  urldate = {2024-01-29},
  abstract = {This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Computer Science - Programming Languages,notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/WBG3LCMC/Baghdadi et al. - 2018 - Tiramisu A Polyhedral Compiler for Expressing Fas.pdf;/Users/yanyucheng/Zotero/storage/SJ5NLCHU/1804.html}
}

@inproceedings{chenLearningOptimizeTensor2018,
  title = {Learning to {{Optimize Tensor Programs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Tianqi and Zheng, Lianmin and Yan, Eddie and Jiang, Ziheng and Moreau, Thierry and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-06-25},
  abstract = {We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/IH73H57W/Chen et al. - 2018 - Learning to Optimize Tensor Programs.pdf}
}

@inproceedings{dingHidetTaskMappingProgramming2023,
  title = {Hidet: {{Task-Mapping Programming Paradigm}} for {{Deep Learning Tensor Programs}}},
  shorttitle = {Hidet},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}, {{Volume}} 2},
  author = {Ding, Yaoyao and Yu, Cody Hao and Zheng, Bojian and Liu, Yizhi and Wang, Yida and Pekhimenko, Gennady},
  year = {2023},
  month = jan,
  pages = {370--384},
  publisher = {{ACM}},
  address = {{Vancouver BC Canada}},
  doi = {10.1145/3575693.3575702},
  urldate = {2023-08-23},
  isbn = {978-1-4503-9916-6},
  langid = {english},
  file = {/Users/yanyucheng/Zotero/storage/8P737I6N/Ding et al. - 2023 - Hidet Task-Mapping Programming Paradigm for Deep .pdf}
}

@inproceedings{gaoSwATOPAutomaticallyOptimizing2019,
  title = {{{swATOP}}: {{Automatically Optimizing Deep Learning Operators}} on {{SW26010 Many-Core Processor}}},
  shorttitle = {{{swATOP}}},
  booktitle = {Proceedings of the 48th {{International Conference}} on {{Parallel Processing}}},
  author = {Gao, Wei and Fang, Jiarui and Zhao, Wenlai and Yang, Jinzhe and Wang, Long and Gan, Lin and Fu, Haohuan and Yang, Guangwen},
  year = {2019},
  month = aug,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Kyoto Japan}},
  doi = {10.1145/3337821.3337883},
  urldate = {2023-05-11},
  isbn = {978-1-4503-6295-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/TZHJGLI8/Gao et al. - 2019 - swATOP Automatically Optimizing Deep Learning Ope.pdf}
}

@article{grosserPolyhedralASTGeneration2015,
  title = {Polyhedral {{AST Generation Is More Than Scanning Polyhedra}}},
  author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert},
  year = {2015},
  month = aug,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {37},
  number = {4},
  pages = {1--50},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/2743016},
  urldate = {2024-02-06},
  abstract = {Abstract mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations.             We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with fine-grained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain.},
  langid = {english},
  keywords = {/unread,notion},
  file = {/Users/yanyucheng/Zotero/storage/UMD2ZU9A/Grosser et al. - 2015 - Polyhedral AST Generation Is More Than Scanning Po.pdf}
}

@article{grosserPolyhedralASTGeneration2015a,
  title = {Polyhedral {{AST Generation Is More Than Scanning Polyhedra}}},
  author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert},
  year = {2015},
  month = jul,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {37},
  number = {4},
  pages = {12:1--12:50},
  issn = {0164-0925},
  doi = {10.1145/2743016},
  urldate = {2024-02-06},
  abstract = {Abstract mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations. We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with fine-grained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain.},
  keywords = {/unread,code generation,index set splitting,notion,Polyhedral compilation,Presburger relations,unrolling},
  file = {/Users/yanyucheng/Zotero/storage/VNILHYRJ/Grosser et al. - 2015 - Polyhedral AST Generation Is More Than Scanning Po.pdf}
}

@inproceedings{hagedornGrapheneIROptimized2023,
  title = {Graphene: {{An IR}} for {{Optimized Tensor Computations}} on {{GPUs}}},
  shorttitle = {Graphene},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}, {{Volume}} 3},
  author = {Hagedorn, Bastian and Fan, Bin and Chen, Hanfeng and Cecka, Cris and Garland, Michael and Grover, Vinod},
  year = {2023},
  month = mar,
  pages = {302--313},
  publisher = {{ACM}},
  address = {{Vancouver BC Canada}},
  doi = {10.1145/3582016.3582018},
  urldate = {2023-12-25},
  isbn = {978-1-4503-9918-0},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/Z7TZWIF5/Hagedorn et al. - 2023 - Graphene An IR for Optimized Tensor Computations .pdf}
}

@article{HuYiGuoChanSW26010ProChuLiQiShang3JiBLASHanShuZhongHeBingXingYouHua,
  title = {{国产SW26010-Pro处理器上3级BLAS函数众核并行优化}},
  author = {胡怡 and 陈道琨 and 杨超 and 马文静 and 刘芳芳 and 宋超博 and 孙强 and 史俊达},
  journal = {软件学报},
  pages = {1--16},
  issn = {1000-9825},
  doi = {10.13328/j.cnki.jos.006811},
  abstract = {BLAS (basic linear algebra subprograms)是最基本、最重要的底层数学库之一.在一个标准的BLAS库中,BLAS 3级函数涵盖的矩阵-矩阵运算尤为重要,在许多大规模科学与工程计算应用中被广泛调用.另外, BLAS 3级属于计算密集型函数,对充分发挥处理器的计算性能有至关重要的作用.针对国产SW26010-Pro处理器研究BLAS 3级函数的众核并行优化技术.具体而言,根据SW26010-Pro的存储层次结构,设计多级分块算法,挖掘矩阵运算的并行性.在此基础上,基于远程内存访问(remote memory access, RMA)机制设计数据共享策略,提高从核间的数据传输效率.进一步,采用三缓冲、参数调优等方法对算法进行全面优化,隐藏直接内存访问(direct memory access, DMA)访存开销和RMA通信开销.此外,利用SW26010-Pro的两条硬件流水线和若干向量化计算/访存指令,还对BLAS 3级函数的矩阵-矩阵乘法、矩阵方程组求解、矩阵转置操作等若干运算进行手工汇编优化,提高了函数的浮点计算效率.实验结果显示,所提出的并行优化技术在SW26010-Pro处理器上为BLAS 3级函数带来了明显的性能提升,单核组BLAS 3级函数的浮点计算性能最高可达峰值性能的92\%,多核组BLAS 3级函数的浮点计算性能最高可达峰值性能的88\%.},
  langid = {chinese},
  keywords = {BLAS 3,direct memory access (DMA),DMA,floating point computing efficiency,level-3 BLAS,notion,remote memory access (RMA),RMA,SW26010-Pro,SW26010-Pro many-core processor},
  annotation = {{$<$}北大核心, EI, CSCD{$>$}},
  file = {/Users/yanyucheng/Zotero/storage/XSEJGDUI/国产 SW26010-Pro 处理器上 3 级 BLAS 函数众核并行优化.pdf}
}

@article{HuYiMianXiangGuoChanSW26010PZhongHeChuLiQiDeBLASKuXingNengYouHuaJiShuYanJiu,
  title = {{面向国产SW26010P众核处理器的BLAS库性能优化技术研究}},
  author = {胡怡},
  langid = {chinese},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/VKYA66N9/面向国产SW26010P众核处理器的BLAS库性能优化技术研究.pdf}
}

@article{HuYiMianXiangSW26010ProDe12JiBLASHanShuZhongHeBingXingYouHuaJiShu,
  title = {{面向SW26010-Pro的1、2级BLAS函数众核并行优化技术}},
  author = {胡怡 and 陈道琨 and 杨超 and 刘芳芳 and 马文静 and 尹万旺 and 袁欣辉 and 林蓉芬},
  journal = {软件学报},
  pages = {1--16},
  issn = {1000-9825},
  doi = {10.13328/j.cnki.jos.006527},
  abstract = {BLAS (basic linear algebra subprograms)是高性能扩展数学库的一个重要模块,广泛应用于科学与工程计算领域. BLAS 1级提供向量-向量运算, BLAS 2级提供矩阵-向量运算.针对国产SW26010-Pro众核处理器设计并实现了高性能BLAS 1、2级函数.基于RMA通信机制设计了从核归约策略,提升了BLAS 1、2级若干函数的归约效率.针对TRSV、TPSV等存在数据依赖关系的函数,提出了一套高效并行算法,该算法通过点对点同步维持数据依赖关系,设计了适用于三角矩阵的高效任务映射机制,有效减少了从核点对点同步的次数,提高了函数的执行效率.通过自适应优化、向量压缩、数据复用等技术,进一步提升了BLAS 1、2级函数的访存带宽利用率.实验结果显示, BLAS 1级函数的访存带宽利用率最高可达95\%,平均可达90\%以上, BLAS 2级函数的访存带宽利用率最高可达98\%,平均可达80\%以上.与广泛使用的开源数学库GotoBLAS相比, BLAS 1、2级函数分别取得了平均18.78倍和25.96倍的加速效果. LU分解、QR分解以及对称特征值问题通过调用本文实现的高性能BLAS 1、2级函数取得了平均10.99倍的加速效果.},
  langid = {chinese},
  keywords = {,adaptive optimization,BLAS 1,BLAS 2,level 1 BLAS,level 2 BLAS,memory access bandwidth,notion,point-to-point synchronization,RMA,RMA communication,Sunway 26010-Pro many-core processor,SW26010-Pro},
  annotation = {{$<$}北大核心, EI, CSCD{$>$}},
  file = {/Users/yanyucheng/Zotero/storage/P3D5LRYV/面向 SW26010-Pro 的 1、2 级 BLAS 函数众核并行优化技术.pdf}
}

@inproceedings{jiangEasyViewEnablingScheduling2023a,
  title = {{{EasyView}}: {{Enabling}} and {{Scheduling Tensor Views}} in {{Deep Learning Compilers}}},
  shorttitle = {{{EasyView}}},
  booktitle = {Proceedings of the 51st {{International Conference}} on {{Parallel Processing}}},
  author = {Jiang, Lijuan and Xu, Ping and Zhu, Qianchao and Li, Xiuhong and Yan, Shengen and Zhang, Xingcheng and Lin, Dahua and Ma, Wenjing and Li, Zhouyang and Liu, Jun and Ma, Jinming and Jin, Minxi and Yang, Chao},
  year = {2023},
  month = jan,
  series = {{{ICPP}} '22},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3545008.3545037},
  urldate = {2023-06-25},
  abstract = {In recent years, memory-intensive operations are becoming dominant in efficiency of running novel neural networks. Just-in-time operator fusion on accelerating devices like GPU proves an effective method for optimizing memory-intensive operations, and suits the numerous varying model structures. In particular, we find memory-intensive operations on tensor views are ubiquitous in neural network implementations. Tensors are the de facto representation for numerical data in deep learning areas, while tensor views cover a bunch of sophisticated syntax, which allow various interpretations on the underlying tensor data without memory copy. The support of views in deep learning compilers could greatly enlarge operator fusion scope, and appeal to optimizing novel neural networks. Nevertheless, mainstream solutions in state-of-the-art deep learning compilers exhibit imperfections either in view syntax representations or operator fusion. In this article, we propose EasyView, which enables and schedules tensor views in an end-to-end workflow from neural networks onto devices. Aiming at maximizing memory utilization and reducing data movement, we categorize various view contexts in high-level language, and lower views in accordance with different scenarios. Reference-semantic in terms of views are kept in the lowering from native high-level language features to intermediate representations. Based on the reserved reference-semantics, memory activities related to data dependence of read and write are tracked for further compute and memory optimization. Besides, ample operator fusion is applied to memory-intensive operations with views. In our tests, the proposed work could get average 5.63X, 2.44X, and 4.67X speedup compared with the XLA, JAX, and TorchScript, respectively for hotspot Python functions. In addition, operation fusion with views could bring 8.02\% performance improvement in end-to-end neural networks.},
  isbn = {978-1-4503-9733-9},
  keywords = {deep learning compilers,memory-intensive operations,notion,operator fusion,view}
}

@article{jiangEnablingHighlyEfficient2020,
  title = {Enabling {{Highly Efficient Batched Matrix Multiplications}} on {{SW26010 Many-core Processor}}},
  author = {Jiang, Lijuan and Yang, Chao and Ma, Wenjing},
  year = {2020},
  month = mar,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {17},
  number = {1},
  pages = {1--23},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/3378176},
  urldate = {2024-01-04},
  abstract = {We present a systematic methodology for optimizing batched matrix multiplications on SW26010 many-core processor of the Sunway TaihuLight supercomputer. Five surrogate algorithms and a machine learning{\textendash}based algorithm selector are proposed to fully exploit the computing capability of SW26010 and cope with the sophisticated algorithm characteristics of batched matrix multiplications. Experiment results show that the algorithm selector is able to adaptively choose the appropriate algorithm for various matrix shapes and batch sizes with low overhead and high accuracy. In particular, the optimized batched matrix multiplications can substantially outperform the non-batched version and reach around 84.8\% of the performance upper bound.},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/QQ2CX79L/Jiang et al. - 2020 - Enabling Highly Efficient Batched Matrix Multiplic.pdf}
}

@misc{liAnalyticalCharacterizationDesign2021,
  title = {Analytical {{Characterization}} and {{Design Space Exploration}} for {{Optimization}} of {{CNNs}}},
  author = {Li, Rui and Xu, Yufan and {Sukumaran-Rajam}, Aravind and Rountev, Atanas and Sadayappan, P.},
  year = {2021},
  month = mar,
  eprint = {2101.09808},
  primaryclass = {cs},
  doi = {10.1145/3445814.3446759},
  urldate = {2024-01-15},
  abstract = {Moving data through the memory hierarchy is a fundamental bottleneck that can limit the performance of core algorithms of machine learning, such as convolutional neural networks (CNNs). Loop-level optimization, including loop tiling and loop permutation, are fundamental transformations to reduce data movement. However, the search space for finding the best loop-level optimization configuration is explosively large. This paper develops an analytical modeling approach for finding the best loop-level optimization configuration for CNNs on multi-core CPUs. Experimental evaluation shows that this approach achieves comparable or better performance than state-of-the-art libraries and auto-tuning based optimizers for CNNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/Users/yanyucheng/Zotero/storage/RESMLGQK/Li et al. - 2021 - Analytical Characterization and Design Space Explo.pdf;/Users/yanyucheng/Zotero/storage/L7X596RC/2101.html}
}

@article{liDifferentiableProgrammingImage2018,
  title = {Differentiable Programming for Image Processing and Deep Learning in Halide},
  author = {Li, Tzu-Mao and Gharbi, Micha{\"e}l and Adams, Andrew and Durand, Fr{\'e}do and {Ragan-Kelley}, Jonathan},
  year = {2018},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {37},
  number = {4},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3197517.3201383},
  urldate = {2023-12-22},
  abstract = {Gradient-based optimization has enabled dramatic advances in computational imaging through techniques like deep learning and nonlinear optimization. These methods require gradients not just of simple mathematical functions, but of general               programs               which encode complex transformations of images and graphical data. Unfortunately, practitioners have traditionally been limited to either hand-deriving gradients of complex computations, or composing programs from a limited set of coarse-grained operators in deep learning frameworks. At the same time, writing programs with the level of performance needed for imaging and deep learning is prohibitively difficult for most programmers.                          We extend the image processing language Halide with general reverse-mode automatic differentiation (AD), and the ability to automatically optimize the implementation of gradient computations. This enables automatic computation of the gradients of arbitrary Halide programs, at high performance, with little programmer effort. A key challenge is to structure the gradient code to retain parallelism. We define a simple algorithm to automatically schedule these pipelines, and show how Halide's existing scheduling primitives can express and extend the key AD optimization of "checkpointing."             Using this new tool, we show how to easily define new neural network layers which automatically compile to high-performance GPU implementations, and how to solve nonlinear inverse problems from computational imaging. Finally, we show how differentiable programming enables dramatically improving the quality of even traditional, feed-forward image processing algorithms, blurring the distinction between classical and deep methods.},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/NC69IFVJ/Li et al. - 2018 - Differentiable programming for image processing an.pdf}
}

@misc{looksDeepLearningDynamic2017,
  title = {Deep {{Learning}} with {{Dynamic Computation Graphs}}},
  author = {Looks, Moshe and Herreshoff, Marcello and Hutchins, DeLesley and Norvig, Peter},
  year = {2017},
  month = feb,
  number = {arXiv:1702.02181},
  eprint = {1702.02181},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.02181},
  urldate = {2023-06-25},
  abstract = {Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion,Statistics - Machine Learning},
  file = {/Users/yanyucheng/Zotero/storage/CSTPN4CY/Looks et al. - 2017 - Deep Learning with Dynamic Computation Graphs.pdf;/Users/yanyucheng/Zotero/storage/8AZQW3VA/1702.html}
}

@article{mankowitzFasterSortingAlgorithms2023,
  title = {Faster Sorting Algorithms Discovered Using Deep Reinforcement Learning},
  author = {Mankowitz, Daniel J. and Michi, Andrea and Zhernov, Anton and Gelmi, Marco and Selvi, Marco and Paduraru, Cosmin and Leurent, Edouard and Iqbal, Shariq and Lespiau, Jean-Baptiste and Ahern, Alex and K{\"o}ppe, Thomas and Millikin, Kevin and Gaffney, Stephen and Elster, Sophie and Broshear, Jackson and Gamble, Chris and Milan, Kieran and Tung, Robert and Hwang, Minjae and Cemgil, Taylan and Barekatain, Mohammadamin and Li, Yujia and Mandhane, Amol and Hubert, Thomas and Schrittwieser, Julian and Hassabis, Demis and Kohli, Pushmeet and Riedmiller, Martin and Vinyals, Oriol and Silver, David},
  year = {2023},
  month = jun,
  journal = {Nature},
  volume = {618},
  number = {7964},
  pages = {257--263},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-023-06004-9},
  urldate = {2023-06-08},
  abstract = {Abstract                            Fundamental algorithms such as sorting or hashing are used trillions of times on any given day               1               . As demand for computation grows, it has become critical for these algorithms to be as performant as possible. Whereas remarkable progress has been achieved in the past               2               , making further improvements on the efficiency of these routines has proved challenging for both human scientists and computational approaches. Here we show how artificial intelligence can go beyond the current state of the art by discovering hitherto unknown routines. To realize this, we formulated the task of finding a better sorting routine as a single-player game. We then trained a new deep reinforcement learning agent, AlphaDev, to play this game. AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks. These algorithms have been integrated into the LLVM standard C++ sort library               3               . This change to this part of the sort library represents the replacement of a component with an algorithm that has been automatically discovered using reinforcement learning. We also present results in extra domains, showcasing the generality of the approach.},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/VH3CATSZ/Mankowitz et al. - 2023 - Faster sorting algorithms discovered using deep re.pdf}
}

@article{muHAOTunerHardwareAdaptive2023,
  title = {{{HAOTuner}}: {{A Hardware Adaptive Operator Auto-Tuner}} for {{Dynamic Shape Tensor Compilers}}},
  shorttitle = {{{HAOTuner}}},
  author = {Mu, Pengyu and Liu, Yi and Wang, Rui and Liu, Guoxiang and Sun, Zhonghao and Yang, Hailong and Luan, Zhongzhi and Qian, Depei},
  year = {2023},
  journal = {IEEE Transactions on Computers},
  pages = {1--13},
  issn = {1557-9956},
  doi = {10.1109/TC.2023.3288758},
  abstract = {Deep learning compilers with auto-tuners have the ability to generate high-performance programs, particularly tensor programs on accelerators. However, the performance of these tensor programs is shape-sensitive and hardware resource-sensitive. When the tensor shape is only known at runtime instead of compile time, auto-tuners must tune the tensor programs for every possible shape, leading to significant time and cost overhead. Additionally, if a tensor program tuned for one device is deployed on a different device, the performance may not be as optimal as before. To address these challenges, we propose HAOTuner, a hardware-adaptive deep learning operator auto-tuner specifically designed for dynamic shape tensors. We leverage the concept of micro-kernels as the unit of task allocation and have observed that the size of the micro-kernel greatly impacts performance. In HAOTuner, we determine the size of micro-kernels based not only on the tensor shapes but also on the available hardware resources. Specifically, we present an algorithm to select hardware-friendly micro-kernels as candidates, reducing the tuning time. We also design a cost model that is sensitive to hardware resources to support various hardware architectures. Furthermore, we provide a model transfer solution to enable fast deployment of the cost model on different hardware platforms. We evaluate HAOTuner on six different types of GPUs. The experiments demonstrate that HAOTuner surpasses the state-of-the-art dynamic shape tensor auto-tuner in terms of running time by an average of 26\% and tuning time by 25\%. Moreover, HAOTuner outperforms the state-of-the-art compiler with padding in terms of running time by an average of 39\% and tuning time by 6{\texttimes}.},
  keywords = {Auto-Tuning,Costs,Deep learning,Deep Learning Compilation,Dynamic Shape Tensor,Hardware,Heuristic algorithms,Shape,Tensor Program,Tensors,Tuning},
  file = {/Users/yanyucheng/Zotero/storage/Y6LBJJ5R/Mu et al. - 2023 - HAOTuner A Hardware Adaptive Operator Auto-Tuner .pdf;/Users/yanyucheng/Zotero/storage/257B5TH3/10160123.html}
}

@article{mullapudiAutomaticallySchedulingHalide2016,
  title = {Automatically Scheduling Halide Image Processing Pipelines},
  author = {Mullapudi, Ravi Teja and Adams, Andrew and Sharlet, Dillon and {Ragan-Kelley}, Jonathan and Fatahalian, Kayvon},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2897824.2925952},
  urldate = {2023-12-22},
  abstract = {The Halide image processing language has proven to be an effective system for authoring high-performance image processing code. Halide programmers need only provide a high-level strategy for mapping an image processing pipeline to a parallel machine (a               schedule               ), and the Halide compiler carries out the mechanical task of generating platform-specific code that implements the schedule. Unfortunately, designing high-performance schedules for complex image processing pipelines requires substantial knowledge of modern hardware architecture and code-optimization techniques. In this paper we provide an algorithm for automatically generating high-performance schedules for Halide programs. Our solution extends the function bounds analysis already present in the Halide compiler to automatically perform locality and parallelism-enhancing global program transformations typical of those employed by expert Halide developers. The algorithm does not require costly (and often impractical) auto-tuning, and, in seconds, generates schedules for a broad set of image processing benchmarks that are performance-competitive with, and often better than, schedules manually authored by expert Halide developers on server and mobile CPUs, as well as GPUs.},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/MYM7X4BQ/Mullapudi et al. - 2016 - Automatically scheduling halide image processing p.pdf}
}

@inproceedings{ragan-kelleyHalideLanguageCompiler2013,
  title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
  shorttitle = {Halide},
  booktitle = {Proceedings of the 34th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {{Ragan-Kelley}, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'e}do and Amarasinghe, Saman},
  year = {2013},
  month = jun,
  series = {{{PLDI}} '13},
  pages = {519--530},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2491956.2462176},
  urldate = {2023-06-24},
  abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
  isbn = {978-1-4503-2014-6},
  keywords = {autotuning,compiler,domain specific language,gpu,image processing,locality,notion,optimization,parallelism,redundant computation,vectorization}
}

@phdthesis{SuXingGaoXingNengChouMiXianXingDaiShuShuXueKuGuanJianJiShuYanJiu2020,
  type = {{博士}},
  title = {{高性能稠密线性代数数学库关键技术研究}},
  author = {苏醒},
  year = {2020},
  abstract = {稠密线性代数数学库是科学与工程计算领域最为基础的软件工具,几乎所有科学计算问题都依赖于矩阵计算这一基本计算形式。在稠密线性代数计算软件栈中,最底层最基础的数学库当属BLAS(Basic Linear Algebra Subprograms)程序库。BLAS选取了一组在数值计算程序中被经常使用的矩阵(向量)操作,为它们制定了规范化的编程接口(API),作为构建科学计算软件的基本模块。经过多年的学术研究与生产实践,BLAS的接口规范得到了学术界与工业界的广泛认可,业已成为事实上的标准程序库。通用矩阵乘法例程(GEneral Matrix Multiply)是BLAS库中最重要的计算例程。研究表明,BLAS库中多数Level-3计算例程的计算部分都可以通过调用GEMM例程完成,即,GEMM例程可以作为整个Level-3BLAS的构建基础。因此,优化GEMM例程便成为高性能BLAS库开发工作的重中之重。当前,计算硬件的发展十分迅速。对通用处理器(CPU)而言,其体系结构呈现出多方面发展趋势,包括单指令多数据(SIMD)并行指令扩展在高性能处理器中的广泛应用,单处理器核心数目持续增长,处理器片上存储层次结构不断多样化、复杂化,内存节点增多导致非一致性内存访问(NUMA)效应进一步凸显等。这些新的发展趋势都为高性能BLAS库的开发带来了新的挑战。本论文面向多核与众核通用处理器,对高性能BLAS库的开发优化展开研究。论文的主要工作包括:(1)提出了一种基于编译器的可移植的GEMM kernel函数优化方法Poca。在当前的BLAS库实现中,为了最大限度地开发处理器的指令级并行能力,GEMM例程中执行浮点计算的kernel函数通常由领域专家使用汇编语言编写。每一款处理器芯片都需要领域专家针对其微体系结构特点编写专用的汇编程序,造成该方法耗费人力成本高,程序可移植性差。Poca的核心思路是利用LLVM编译器中对不同处理器微体系结构建立的统一抽象模型,提出一套kernel函数的自动生成与优化过程。该优化过程与具体的处理器平台无关,使得Poca方法具有良好的平台移植性。并且,通过采用与领域专家编写汇编程序相似的优化技术,Poca方法可以达到与专家汇编程序相当甚至更优的程序性能。(2)提出了一种针对非LRU(Least Recently Used)替换策略共享cache的cache划分策略SCP。现有的GEMM实现通常假设处理器核的L1 cache与L2cache皆为私有,并且采用LRU替换策略。但随着处理器体系结构发展,出现了使用非LRU共享cache的高性能处理器。在这样的处理器平台上,共享同一cache的不同线程之间会产生大量的cache冲突造成cache缺失率(miss rate)上升,程序性能下降。SCP方法将共享cache的存储空间划分成物理上互不相交的子空间,通过cache自身的地址映射机制保证线程的私有数据存放在各自的子空间中,从而有效避免了线程间的cache数据冲突。(3)提出了一种针对NUMA体系结构的混合粒度动态负载均衡方法。由于GEMM计算的规则性,其并行实现一般采用粗粒度的并行策略,将计算负载平均地划分给参与计算的所有线程。在NUMA体系结构上,内存访问的NUMA特性会导致线程执行速度出现不一致的现象,这会使得线程间数据同步开销增加,而GEMM整体性能将受制于最慢的线程。本文提出的混合粒度动态负载均衡方法是一种为GEMM例程专门设计的work-stealing算法,它采用一种粗细粒度结合的负载划分策略,在运行时允许快速线程窃取慢速线程的工作负载以降低线程同步开销。此外,该方法利用GEMM的问题特点,完全避免了队列、树与锁的使用,几乎不引入额外开销。},
  collaborator = {廖湘科},
  langid = {chinese},
  school = {国防科技大学},
  keywords = {,Compiler,High Performance Computing,Linear Algebra,Matrix Multiply,notion,Parallelization},
  annotation = {4 citations(CNKI)[6-25-2023]},
  file = {/Users/yanyucheng/Zotero/storage/RMJRJ6JD/高性能稠密线性代数数学库关键技术研究.pdf}
}

@inproceedings{tannerTensileAutoTuningGEMM2018,
  title = {Tensile: {{Auto-Tuning GEMM GPU Assembly}} for {{All Problem Sizes}}},
  shorttitle = {Tensile},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Tanner, David E.},
  year = {2018},
  month = may,
  pages = {1066--1075},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  doi = {10.1109/IPDPSW.2018.00165},
  urldate = {2023-05-11},
  abstract = {Generic matrix matrix multiplication (GEMM) on graphics processors (GPU) has long been the target of both tuning to find a fastest kernel for a GPU and hand-written assembly to achieve highest possible efficiency. Tensile is an open source (https://github.com/ROCmSoftwarePlatform/Tensile) tool which builds on both of these, by auto generating assembly kernels, auto benchmarking the kernels against a range of problem sizes, then auto generating library C code which selects the optimal kernel for a given problem size. Not only can the kernels achieve near-peak efficiency for the best problem sizes, but the size-tuned kernels can achieve speedups of 2X to 350X compared to any single-tuned kernel optimized for a given GPU architecture.},
  isbn = {978-1-5386-5555-9},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/GB99EH65/Tanner - 2018 - Tensile Auto-Tuning GEMM GPU Assembly for All Pro.pdf}
}

@inproceedings{taoAutomaticallyGeneratingHighperformance2022,
  title = {Automatically {{Generating High-performance Matrix Multiplication Kernels}} on the {{Latest Sunway Processor}}},
  booktitle = {Proceedings of the 51st {{International Conference}} on {{Parallel Processing}}},
  author = {Tao, Xiaohan and Zhu, Yu and Wang, Boyang and Xu, Jinlong and Pang, Jianmin and Zhao, Jie},
  year = {2022},
  month = aug,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Bordeaux France}},
  doi = {10.1145/3545008.3545031},
  urldate = {2023-05-11},
  abstract = {We present an approach to the automatic generation of efficient matrix multiplication code on the latest Sunway processor, which will be employed by the next-generation machine of Sunway TaihuLight, one of the fastest supercomputers on earth. The method allows users to write simple C code and automatically generates high-performance matrix multiplication kernels. It uses polyhedral transformations to implement rapid compute decomposition, data exchanges across memory hierarchy and memory latency hiding. An assembly routine is finally integrated into the generated kernels. While achieving up to 90.14\% of the theoretical peak performance, our method surpasses a highly tuned library by 9.44\%. Compared with existing techniques, our approach reduces the software development life cycle to generate efficient matrix code from months to seconds. We also take into account batched matrix multiplication and some fusion patterns for deep learning (DL), outperforming the library-based implementations by 1.30{\texttimes} and 1.67{\texttimes}.},
  isbn = {978-1-4503-9733-9},
  langid = {english},
  keywords = {notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/MRL4KCMQ/Tao et al. - 2022 - Automatically Generating High-performance Matrix M.pdf}
}

@misc{vasilacheTensorComprehensionsFrameworkAgnostic2018,
  title = {Tensor {{Comprehensions}}: {{Framework-Agnostic High-Performance Machine Learning Abstractions}}},
  shorttitle = {Tensor {{Comprehensions}}},
  author = {Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S. and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  year = {2018},
  month = jun,
  number = {arXiv:1802.04730},
  eprint = {1802.04730},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.04730},
  urldate = {2023-06-25},
  abstract = {Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/3LNPIJVY/Vasilache et al. - 2018 - Tensor Comprehensions Framework-Agnostic High-Per.pdf;/Users/yanyucheng/Zotero/storage/66DU7SHC/1802.html}
}

@article{verdoolaegePolyhedralParallelCode2013,
  title = {Polyhedral Parallel Code Generation for {{CUDA}}},
  author = {Verdoolaege, Sven and Carlos Juega, Juan and Cohen, Albert and Ignacio G{\'o}mez, Jos{\'e} and Tenllado, Christian and Catthoor, Francky},
  year = {2013},
  month = jan,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {9},
  number = {4},
  pages = {1--23},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/2400682.2400713},
  urldate = {2023-06-02},
  abstract = {This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary. We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs. We evaluate our algorithms and tool on the entire PolyBench suite.},
  langid = {english},
  keywords = {notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/W3QTZUV3/Verdoolaege et al. - 2013 - Polyhedral parallel code generation for CUDA.pdf}
}

@article{verdoolaegePresburgerFormulasPolyhedral,
  title = {Presburger {{Formulas}} and {{Polyhedral Compilation}}},
  author = {Verdoolaege, Sven and Labs, Polly and Leuven, {\relax KU}},
  langid = {english},
  keywords = {notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/Y9CN4IBB/Verdoolaege et al. - Presburger Formulas and Polyhedral Compilation.pdf}
}

@inproceedings{wangAUGEMAutomaticallyGenerate2013a,
  title = {{{AUGEM}}: {{Automatically}} Generate High Performance {{Dense Linear Algebra}} Kernels on X86 {{CPUs}}},
  shorttitle = {{{AUGEM}}},
  booktitle = {{{SC}} '13: {{Proceedings}} of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Wang, Qian and Zhang, Xianyi and Zhang, Yunquan and Yi, Qing},
  year = {2013},
  month = nov,
  pages = {1--12},
  issn = {2167-4337},
  doi = {10.1145/2503210.2503219},
  urldate = {2023-12-25},
  abstract = {Basic Liner algebra subprograms (BLAS) is a fundamental library in scientific computing. In this paper, we present a template-based optimization framework, AUGEM, which can automatically generate fully optimized assembly code for several dense linear algebra (DLA) kernels, such as GEMM, GEMV, AXPY and DOT, on varying multi-core CPUs without requiring any manual interference from developers. In particular, based on domain-specific knowledge about algorithms of the DLA kernels, we use a collection of parameterized code templates to formulate a number of commonly occurring instruction sequences within the optimized low-level C code of these DLA kernels. Then, our framework uses a specialized low-level C optimizer to identify instruction sequences that match the pre-defined code templates and thereby translates them into extremely efficient SSE/AVX instructions. The DLA kernels generated by our templatebased approach surpass the implementations of Intel MKL and AMD ACML BLAS libraries, on both Intel Sandy Bridge and AMD Piledriver processors.},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/8DQ5PMTX/6877458.html}
}

@misc{wanghsMianXiangShenWeiZhongHeChuLiQiDeJuZhenDanBianFenJieSuanFaSheJiYuShiXian,
  title = {面向申威众核处理器的矩阵单边分解算法设计与实现},
  author = {{wang,hs}},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/54E7P5SE/面向申威众核处理器的矩阵单边分解算法设计与实现.pdf}
}

@inproceedings{xuHighPerformanceMatrixMultiplication2022,
  title = {High-{{Performance Matrix Multiplication}} on the {{New Generation Shenwei Processor}}},
  booktitle = {2022 {{IEEE}} 24th {{Int Conf}} on {{High Performance Computing}} \& {{Communications}}; 8th {{Int Conf}} on {{Data Science}} \& {{Systems}}; 20th {{Int Conf}} on {{Smart City}}; 8th {{Int Conf}} on {{Dependability}} in {{Sensor}}, {{Cloud}} \& {{Big Data Systems}} \& {{Application}} ({{HPCC}}/{{DSS}}/{{SmartCity}}/{{DependSys}})},
  author = {Xu, Le and An, Hong and Chen, Junshi and Zhang, Pengfei},
  year = {2022},
  month = dec,
  pages = {894--903},
  publisher = {{IEEE}},
  address = {{Hainan, China}},
  doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00145},
  urldate = {2023-12-25},
  abstract = {As a critical operation in numerical computing, matrix multiplication is widely used in many high-performance applications. The new generation Shenwei processor is the newest many-core processor with powerful computing capacity, thus it's necessary to enable highly efficient matrix multiplication on it. In this paper, we present a detailed implementation of double-precision matrix multiplication on Shenwei processor and further perform deep optimizations in terms of both memory access and computing. We employ a three-level task partitioning scheme mapping blocked matrix multiplication to each memory hierarchy to improve computational parallelism and data reusability. And the dual broadcasting algorithm enables efficient parallel computing on chip. For further optimization, we present a three-level latency hiding strategy to bridge the performance gap between computing and memory access, where the hybrid data prefetch effectively hides DMA memory access and RMA communication costs. And the adaptive blocking algorithm is proposed for sake of the generality of algorithms towards various matrix sizes and shapes. High-performance matrix multiplication is highly correlated with the compute kernel, and we propose the register-buffering instruction scheduling with latency hiding strategy to maximize pipeline efficiency. The experiments show that the performance of our algorithm can reach 2 TFLOPS at large scales, near the theoretical peak performance. And the hybrid prefetch can improve performance by 82\%, while the adaptive blocking not only improves performance but also smoothes the performance jitter of irregular matrix multiplications. The compute kernel with instruction scheduling is highly optimized and can achieve more than 380\% performance improvement.},
  isbn = {9798350319934},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/GU7PSJ98/Xu et al. - 2022 - High-Performance Matrix Multiplication on the New .pdf}
}

@article{zhaoAKGAutomaticKernel2021,
  title = {{{AKG}}: {{Automatic Kernel Generation}} for {{Neural Processing Units}} Using {{Polyhedral Transformations}}},
  author = {Zhao, Jie and Li, Bojie and Nie, Wang and Geng, Zhen and Zhang, Renwei and Gao, Xiong and Cheng, Bin and Wu, Chen and Cheng, Yun and Li, Zheng and Di, Peng and Zhang, Kun and Jin, Xuefeng},
  year = {2021},
  abstract = {Existing tensor compilers have proven their effectiveness in deploying deep neural networks on general-purpose hardware like CPU and GPU, but optimizing for neural processing units (NPUs) is still challenging due to the heterogeneous compute units and complicated memory hierarchy.},
  langid = {english},
  keywords = {AKG,notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/ZFA9H5GA/Zhao et al. - 2021 - AKG Automatic Kernel Generation for Neural Proces.pdf}
}

@inproceedings{zhaoOptimizingMemoryHierarchy2020,
  title = {Optimizing the {{Memory Hierarchy}} by {{Compositing Automatic Transformations}} on {{Computations}} and {{Data}}},
  booktitle = {2020 53rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Zhao, Jie and Di, Peng},
  year = {2020},
  month = oct,
  pages = {427--441},
  publisher = {{IEEE}},
  address = {{Athens, Greece}},
  doi = {10.1109/MICRO50266.2020.00044},
  urldate = {2024-01-30},
  abstract = {Optimizing compilers exploit the memory hierarchy using loop tiling and fusion, but these two transformations usually interfere with each other due to the oversight of transformations on data in memories. We present a novel composition of loop tiling and fusion in this paper. Unlike existing tiling-afterfusion algorithms that only transform computation spaces, our approach first applies rectangular/parallelogram tiling to live-out computation spaces for fitting the memory hierarchy, followed by the computation of the memory footprints required by each tile. The upwards exposed data extracted from the memory footprints are used to determine the tile shapes of intermediate computation spaces, allowing the construction of arbitrary tile shapes. Finally, our technique implements a post-tiling fusion strategy for maximizing data locality without losing tilability or parallelism of live-out computation spaces, thereby enabling storage reduction and reuse, and optimizing the memory hierarchy. We demonstrate that our approach can achieve superior performance on both CPU and GPU architectures over the state of the art by experimenting on 11 benchmarks extracted from numerous domains including neural networks, image processing, sparse matrix computation and linear algebra. Also, the results of the ResNet-50 model on an AI accelerator show that our approach can obtain 16\% performance improvement.},
  isbn = {978-1-72817-383-2},
  langid = {english},
  keywords = {AKG,notion,poly},
  file = {/Users/yanyucheng/Zotero/storage/E5F3FWAS/Zhao and Di - 2020 - Optimizing the Memory Hierarchy by Compositing Aut.pdf}
}

@misc{zhengAnsorGeneratingHighPerformance2020,
  title = {Ansor : {{Generating High-Performance Tensor Programs}} for {{Deep Learning}}},
  shorttitle = {Ansor},
  author = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and {Haj-Ali}, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2020},
  month = nov,
  number = {arXiv:2006.06762},
  eprint = {2006.06762},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Computer Science - Programming Languages,notion,Statistics - Machine Learning},
  file = {/Users/yanyucheng/Zotero/storage/MCT6ZEBX/Zheng et al. - 2020 - Ansor  Generating High-Performance Tensor Program.pdf}
}

@article{zhengAStitchEnablingNew2022,
  title = {{{AStitch}}: {{Enabling}} a {{New Multi-dimensional Optimization Space}} for {{Memory-Intensive ML Training}} and {{Inference}} on {{Modern SIMT Architectures}}},
  author = {Zheng, Zhen and Yang, Xuanda and Zhao, Pengzhan and Long, Guoping and Zhu, Kai and Zhu, Feiwen and Zhao, Wenyi and Liu, Xiaoyong and Yang, Jun and Zhai, Jidong and Song, Shuaiwen Leon and Lin, Wei},
  year = {2022},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/5YNQVS3G/Zheng et al. - 2022 - AStitch Enabling a New Multi-dimensional Optimiza.pdf}
}

@misc{zhengFusionStitchingBoostingMemory2021,
  title = {{{FusionStitching}}: {{Boosting Memory Intensive Computations}} for {{Deep Learning Workloads}}},
  shorttitle = {{{FusionStitching}}},
  author = {Zheng, Zhen and Zhao, Pengzhan and Long, Guoping and Zhu, Feiwen and Zhu, Kai and Zhao, Wenyi and Diao, Lansong and Yang, Jun and Lin, Wei},
  year = {2021},
  month = dec,
  number = {arXiv:2009.10924},
  eprint = {2009.10924},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.10924},
  urldate = {2023-06-25},
  abstract = {We show in this work that memory intensive computations can result in severe performance problems due to off-chip memory access and CPU-GPU context switch overheads in a wide range of deep learning models. For this problem, current just-in-time (JIT) kernel fusion and code generation techniques have limitations, such as rough fusion plan exploration strategies and limited code generation ability. We propose FusionStitching, a deep learning compiler capable of fusing memory intensive operators, with varied data dependencies and non-homogeneous parallelism, into large GPU kernels to reduce global memory access and context switch overhead automatically. FusionStitching widens the range of operation combinations that fusion can target beyond previous JIT works by introducing data reuse of intermediate values. It explores large fusion spaces to decide optimal fusion plans with considerations of memory access costs, kernel calls and resource usage constraints. FusionStitching tunes the optimal stitching scheme with a domain-specific cost model efficiently. Experimental results show that FusionStitching can reach up to 2.21x speedup compared to state-of-the-art, with 1.45x on average. Besides these experimental results, we integrated our approach into a compiler product and deployed it onto a production cluster for AI workloads with thousands of GPUs. The system has been in operation for more than 4 months and saves 7,000 GPU hours on average for approximately 30,000 tasks per month.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,notion},
  file = {/Users/yanyucheng/Zotero/storage/3SJWB95P/Zheng et al. - 2021 - FusionStitching Boosting Memory Intensive Computa.pdf;/Users/yanyucheng/Zotero/storage/XG8R5G2Q/2009.html}
}

% reference
@misc{ArmPerformanceLibrariesa,
  title = {Arm {{Performance Libraries}}},
  urldate = {2023-12-25},
  howpublished = {https://developer.arm.com/Tools\%20and\%20Software/Arm\%20Performance\%20Libraries},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/2RRT2HY6/Arm Performance Libraries.html}
}

@misc{CuBLAS,
  title = {{{cuBLAS}}},
  urldate = {2023-12-25},
  howpublished = {https://docs.nvidia.com/cuda/cublas/index.html\#using-the-cublasxt-api},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/BGFZQE2J/index.html}
}

@misc{DenseLinearAlgebra,
  title = {Dense {{Linear Algebra}}: {{AOCL-BLAS}} and {{AOCL-LAPACK}}},
  urldate = {2023-12-25},
  howpublished = {https://www.amd.com/en/developer/aocl/dense.html},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/BRT6DPI2/dense.html}
}

@misc{IntelOneAPIMatha,
  title = {{{Intel}}{\textregistered} {{oneAPI Math Kernel Library}} - {{Data Parallel C}}++ {{Developer}}...},
  journal = {Intel},
  urldate = {2023-12-25},
  abstract = {Intel{\textregistered} oneAPI Math Kernel Library Developer Reference for Data Parallel C++},
  howpublished = {https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-dpcpp/2024-0/overview.html},
  langid = {english},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/ZNZGNLR5/overview.html}
}

@misc{MAGMA,
  title = {{{MAGMA}}},
  urldate = {2023-12-25},
  howpublished = {https://icl.utk.edu/magma/},
  keywords = {notion}
}

@misc{MindSporeGuanWang,
  title = {{MindSpore官网}},
  urldate = {2023-12-22},
  abstract = {华为开源自研AI框架MindSpore。自动微分、并行加持，一次训练，可多场景部署。支持端边云全场景的深度学习训练推理框架，主要应用于计算机视觉、自然语言处理等AI领域，面向数据科学家、算法工程师等人群。主要具备基于源码转换的通用自动微分、自动实现分布式并行训练、数据处理、以及图执行引擎等功能特性。借助自动微分，轻松训练神经网络。框架开源，华为培育AI开发生态。},
  howpublished = {https://www.mindspore.cn/},
  langid = {chinese},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/GUEBYU63/www.mindspore.cn.html}
}

@misc{OpenBLASOptimizedBLAS,
  title = {{{OpenBLAS}} : {{An}} Optimized {{BLAS}} Library},
  urldate = {2023-12-25},
  howpublished = {https://www.openblas.net/},
  keywords = {notion}
}

@misc{RocBLASDocumentationRocBLAS,
  title = {{{rocBLAS Documentation}} --- {{rocBLAS Documentation}}},
  urldate = {2023-12-25},
  howpublished = {https://rocm.docs.amd.com/projects/rocBLAS/en/latest/},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/VZHMHCTX/latest.html}
}

@misc{SetLevelBasic,
  title = {A Set of Level 3 Basic Linear Algebra Subprograms {\textbar} {{ACM Transactions}} on {{Mathematical Software}}},
  urldate = {2023-12-25},
  howpublished = {https://dl.acm.org/doi/10.1145/77626.79170},
  keywords = {notion},
  file = {/Users/yanyucheng/Zotero/storage/SX4T5RV4/77626.html}
}

@article{UpdatedSetBasic2002,
  title = {An Updated Set of Basic Linear Algebra Subprograms ({{BLAS}})},
  year = {2002},
  month = jun,
  journal = {ACM Transactions on Mathematical Software},
  volume = {28},
  number = {2},
  pages = {135--151},
  issn = {0098-3500},
  doi = {10.1145/567806.567807},
  urldate = {2024-03-04},
  keywords = {/unread,BLAS,linear algebra,notion,standards},
  file = {/Users/yanyucheng/Zotero/storage/XCQUGSYG/2002 - An updated set of basic linear algebra subprograms.pdf}
}

@article{vanzeeBLISFrameworkRapidly2015a,
  title = {{{BLIS}}: {{A Framework}} for {{Rapidly Instantiating BLAS Functionality}}},
  shorttitle = {{{BLIS}}},
  author = {Van Zee, Field G. and {van de Geijn}, Robert A.},
  year = {2015},
  month = jun,
  journal = {ACM Transactions on Mathematical Software},
  volume = {41},
  number = {3},
  pages = {14:1--14:33},
  issn = {0098-3500},
  doi = {10.1145/2764454},
  urldate = {2023-12-25},
  abstract = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL).},
  keywords = {BLAS,high-performance,libraries,Linear algebra,matrix,notion}
}
